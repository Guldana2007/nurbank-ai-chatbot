from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import faiss
import json
import numpy as np
import subprocess
from sentence_transformers import SentenceTransformer
import time
import re
from typing import List, Dict

# ---------------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------------
INDEX_FILE = "../embeddings_bge3/index.faiss"
CHUNKED_FILE = "../data/chunked_data.json"

EMBED_MODEL = "BAAI/bge-m3"
USE_GPU = False
LLM_MODEL = "llama3"
TOP_K = 7
MAX_CONTEXT_CHARS = 20000
OLLAMA_TIMEOUT = 600
LOG_SNIPPET_LEN = 200
FINAL_NOTE = "–ü—Ä–æ—à—É –æ–±—Ä–∞—â–∞—Ç—å—Å—è –≤ –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã–π —Ü–µ–Ω—Ç—Ä –ù—É—Ä–±–∞–Ω–∫–∞: +7 727 244 44 44 –∏–ª–∏ –ø–æ WhatsApp: +7 707 000 25 52 (https://wa.me/77070002552)."

# --------------------------------------------

print("[–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è] –ó–∞–≥—Ä—É–∂–∞–µ–º FAISS –∏ –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (BGE3)...")
start = time.time()

index = faiss.read_index(INDEX_FILE)
with open(CHUNKED_FILE, "r", encoding="utf-8") as f:
    docs: List[Dict] = json.load(f)

embed_model = SentenceTransformer(EMBED_MODEL, device="cuda" if USE_GPU else "cpu")
print(f"[–ì–æ—Ç–æ–≤–æ] –ó–∞–ø—É—Å–∫ –∑–∞–Ω—è–ª {time.time() - start:.2f} —Å–µ–∫.")

# ‚úÖ Swagger docs –≤–∫–ª—é—á–µ–Ω—ã
app = FastAPI(
    title="Nurbank ChatBot (Lang Strict, BGE3)",
    description="AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ù—É—Ä–±–∞–Ω–∫–∞ –Ω–∞ –±–∞–∑–µ FAISS + Ollama",
    version="1.0.0"
)

# ‚úÖ CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class Query(BaseModel):
    question: str

# ---------------- utils ----------------
def detect_language(text: str) -> str:
    t = text.lower()
    if any(c in t for c in "”ô“ì“õ“£”©“±“Ø—ñ"):
        return "kk"
    if re.search(r"[–∞-—è—ë]", t):
        return "ru"
    return "en"

def embed_query(text: str):
    q = f"query: {text}"
    v = embed_model.encode([q], normalize_embeddings=True, convert_to_numpy=True)
    return v

def retrieve_context(question: str, lang: str, top_k: int = TOP_K):
    q_vec = embed_query(question)
    D, I = index.search(np.array(q_vec), top_k * 5)

    def url_ok(url: str):
        if any(x in url for x in ["vacancies", "tenders", "/search/"]):
            return False
        if lang == "ru":
            return "/ru/" in url
        if lang == "kk":
            return "/kz/" in url or "/kk/" in url
        return "/ru/" in url

    results = []
    for i in I[0]:
        if i == -1:
            continue
        doc = docs[i]
        url = doc.get("url", "")
        if url_ok(url):
            results.append(doc)
        if len(results) >= top_k:
            break
    return results

def ask_ollama(prompt: str) -> str:
    print(f"[PROMPT] >>>\n{prompt}\n<<<")
    try:
        result = subprocess.run(
            ["ollama", "run", LLM_MODEL],
            input=prompt.encode("utf-8"),
            capture_output=True,
            timeout=OLLAMA_TIMEOUT
        )
        output = result.stdout.decode("utf-8").strip()
        print(f"[OLLAMA OUTPUT] >>>\n{output}\n<<<")
        return output
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ Ollama: {e}"

def filter_by_context(answer: str, context: str) -> str:
    context_words = set(re.findall(r"\w+", context.lower()))
    answer_tokens = re.findall(r"\w+|\W+", answer)
    filtered_tokens = []
    for token in answer_tokens:
        if re.match(r"\w+", token):
            if token.lower() in context_words:
                filtered_tokens.append(token)
        else:
            filtered_tokens.append(token)
    return "".join(filtered_tokens)

# ---------------- API ----------------
@app.get("/")
def home():
    return {"message": "Hello! NurBank AI Assistant is running üöÄ"}

@app.post("/ask")
def ask_bot(query: Query):
    lang = detect_language(query.question)

    lang_hint = {
        "ru": (
            "–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, —Å—Ç—Ä–æ–≥–æ –ø–æ –ø—Ä–∏–≤–µ–¥—ë–Ω–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É –∏–∑ –ù—É—Ä–±–∞–Ω–∫–∞. "
            "–ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã –∏–∑ —ç—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞. –ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏—á–µ–≥–æ –æ—Ç —Å–µ–±—è. "
            "–ù–µ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å –∏ –Ω–µ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è ‚Äî –ø–∏—à–∏ –∏—Ö —Å—Ç—Ä–æ–≥–æ –≤ —Ç–æ–º –≤–∏–¥–µ, "
            "–≤ –∫–æ—Ç–æ—Ä–æ–º –æ–Ω–∏ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ —Ç–µ–∫—Å—Ç–µ. –ù–ï –∏—Å–∫–∞–∂–∞–π –Ω–∞–∑–≤–∞–Ω–∏–µ –±–∞–Ω–∫–∞: –ø–∏—à–∏ —Å—Ç—Ä–æ–≥–æ ‚Äî '–ù—É—Ä–±–∞–Ω–∫'. "
            "–ò–≥–Ω–æ—Ä–∏—Ä—É–π –≤–∞–∫–∞–Ω—Å–∏–∏, —Ç–µ–Ω–¥–µ—Ä—ã, –°–ú–ò –∏ –≤–Ω–µ—à–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏."
        ),
        "kk": (
            "–¢–µ–∫ –º”ô—Ç—ñ–Ω –±–æ–π—ã–Ω—à–∞, —Ç–µ–∫ –Ω–∞“õ—Ç—ã –¥–µ—Ä–µ–∫—Ç–µ—Ä–º–µ–Ω “õ–∞–∑–∞“õ —Ç—ñ–ª—ñ–Ω–¥–µ –∂–∞—É–∞–ø –±–µ—Ä. "
            "–ê—Ç–∞—É–ª–∞—Ä–¥—ã –∞—É–¥–∞—Ä–º–∞ –∂”ô–Ω–µ ”©–∑–≥–µ—Ä—Ç—É–≥–µ –±–æ–ª–º–∞–π–¥—ã ‚Äî –º”ô—Ç—ñ–Ω–¥–µ “õ–∞–ª–∞–π –∂–∞–∑—ã–ª—Å–∞, —Å–æ–ª–∞–π –∂–∞–∑. "
            "'–ù—É—Ä–±–∞–Ω–∫' –∞—Ç–∞—É—ã–Ω ”©–∑–≥–µ—Ä—Ç–ø–µ–π “õ–æ–ª–¥–∞–Ω. –ë–ê“ö, –∂“±–º—ã—Å –æ—Ä—ã–Ω–¥–∞—Ä—ã –º–µ–Ω —Ç–µ–Ω–¥–µ—Ä–ª–µ—Ä–¥—ñ –µ–ª–µ–º–µ."
        ),
        "en": (
            "Answer strictly in English, based ONLY on the provided text from Nurbank. "
            "Do not translate or paraphrase names ‚Äî use them exactly as in the text. "
            "Do not distort the name ‚Äî write strictly 'Nurbank'. Ignore vacancies, tenders, media, and external sources."
        )
    }.get(lang, "Answer strictly and only from the provided text.")

    context_items = retrieve_context(query.question, lang, TOP_K)

    if not context_items or sum(len(c["content"]) for c in context_items) < 300:
        fallback_msg = {
            "ru": "–ò–∑–≤–∏–Ω–∏—Ç–µ, —Ç–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.",
            "kk": "–ö–µ—à—ñ—Ä—ñ“£—ñ–∑, –Ω–∞“õ—Ç—ã –∞“õ–ø–∞—Ä–∞—Ç —Ç–∞–±—ã–ª–º–∞–¥—ã.",
            "en": "Sorry, we couldn't find a precise answer."
        }.get(lang, "Information not found.")
        return {"answer": f"{fallback_msg} {FINAL_NOTE}"}

    print("[DEBUG] –í–´–ë–†–ê–ù–ù–´–ï –î–û–ö–£–ú–ï–ù–¢–´:")
    for i, c in enumerate(context_items, 1):
        snippet = c["content"][:LOG_SNIPPET_LEN].replace("\n", " ")
        print(f" {i}. {c.get('url', '')} [{len(c['content'])} chars] :: {snippet}...")

    context = "\n\n---\n\n".join([c["content"][:MAX_CONTEXT_CHARS] for c in context_items])
    used_urls = [c["url"] for c in context_items]

    prompt = f"""{lang_hint}

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å:
{query.question}

–û—Ç–≤–µ—Ç:"""

    answer = ask_ollama(prompt)
    answer = answer.replace("\n", " ").replace("*", "").replace("\\", "").strip()
    answer = filter_by_context(answer, context)

    # –û—á–∏—Å—Ç–∫–∞
    answer = re.sub(r"(–ù—É—Ä–±–∞–Ω–∫[–∞-—è–ê-–Øa-zA-Z\s\d:+().-]+WhatsApp[^\s]*)", "", answer)
    answer = re.sub(r"—Å–æ–≥–ª–∞—Å–Ω–æ —Ç–µ–∫—Å—Ç—É[, ]*", "", answer, flags=re.IGNORECASE)
    answer = re.sub(r"\s+", " ", answer).strip()

    answer += f" {FINAL_NOTE}"

    return {"answer": answer, "used_urls": used_urls}

@app.get("/health")
def health():
    return {"status": "ok"}
